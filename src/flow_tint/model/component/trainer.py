import signal
import sys
from datetime import datetime
from importlib.metadata import version
from logging import Logger
from pathlib import Path
from typing import cast

import torch
from torch import device as device_type, Tensor
from torch.nn import functional as F
from torch.nn.utils import clip_grad_norm_
from torch.optim import Optimizer
from torch.optim.lr_scheduler import LRScheduler, ReduceLROnPlateau
from torch.utils.data import DataLoader
from tqdm import tqdm

from src.flow_tint.path import get_models_dir, get_logs_dir
from src.flow_tint.model.component.batch_sampler import LengthGroupedBatchSampler
from src.flow_tint.model.component.color_model import ColorModel
from src.flow_tint.model.component.dropout_scheduler import DropoutScheduler
from src.flow_tint.model.component.history import History
from src.flow_tint.model.component.tokenizer import Tokenizer


def compute_loss(logits: Tensor, target: Tensor, mask: Tensor) -> Tensor:
    """
    è®¡ç®—åºåˆ—ç”ŸæˆæŸå¤±

    :param logits [batch_size, seq_len, vocab_size]
    :param target [batch_size, seq_len]
    :param mask [batch_size, seq_len] Trueä¸ºPaddingä½ç½®
    """

    batch_size, seq_len, vocab_size = logits.shape

    # é‡æ–°æŒ‡å®šæ•°æ®è¯»å–ç»´åº¦
    logits_view = logits.view(-1, vocab_size)
    target_view = target.view(-1)
    mask_view = mask.view(-1)

    # è®¡ç®—æŸå¤±
    loss = F.cross_entropy(logits_view, target_view, reduction='none', label_smoothing=0.05)

    # å¿½ç•¥Padding
    logs = loss * (~mask_view).float()

    # å¹³å‡æŸå¤±
    valid_tokens = (~mask_view).sum()
    if valid_tokens > 0:
        return logs.sum() / valid_tokens
    else:
        return logs.sum()


def compute_metrics(logits: Tensor, target: Tensor, mask: Tensor) -> dict[str, float]:
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""

    # è·å–é¢„æµ‹
    predictions = logits.argmax(dim=-1)

    # å¿½ç•¥paddingä½ç½®
    valid_mask = ~mask

    # Tokençº§åˆ«å‡†ç¡®ç‡
    token_correct = (predictions == target) & valid_mask
    token_accuracy = token_correct.sum().float() / valid_mask.sum().float()

    # åºåˆ—çº§åˆ«å‡†ç¡®ç‡
    seq_correct = (token_correct | mask).all(dim=1)
    seq_accuracy = seq_correct.float().mean()

    return {
        "token_accuracy": token_accuracy.item(),
        "sequence_accuracy": seq_accuracy.item(),
        "valid_tokens": valid_mask.sum().item(),
    }


class Trainer:
    # stateless
    model: ColorModel
    train_loader: DataLoader
    val_loader: DataLoader
    device: device_type
    tokenizer: Tokenizer
    log_interval: int
    logger: Logger

    # stateful
    optimizer: Optimizer
    lr_scheduler: LRScheduler
    dropout_scheduler: DropoutScheduler
    history: History
    best_val_loss: float
    start_epoch: int

    # fixed
    epochs = 50
    early_stopping_counter = 0
    early_stopping_patience = 5
    model_dir = get_models_dir()
    checkpoint_dir = model_dir / "checkpoints"
    log_dir = get_logs_dir()

    def __init__(self):
        raise RuntimeError("è¯·ä½¿ç”¨ from_scratch() æˆ– from_checkpoint() åˆ›å»º Trainer å®ä¾‹")

    @staticmethod
    def _init_base(
            model: ColorModel,
            train_loader: DataLoader,
            val_loader: DataLoader,
            device: device_type,
            tokenizer: Tokenizer,
            log_interval: int,
            logger: Logger,
    ) -> "Trainer":
        trainer = object.__new__(Trainer)

        trainer.model = model.to(device)
        trainer.train_loader = train_loader
        trainer.val_loader = val_loader
        trainer.device = device
        trainer.tokenizer = tokenizer
        trainer.log_interval = log_interval
        trainer.logger = logger

        return trainer

    @staticmethod
    def from_scratch(
            model: ColorModel,
            train_loader: DataLoader,
            val_loader: DataLoader,
            device: device_type,
            tokenizer: Tokenizer,
            optimizer: Optimizer,
            lr_scheduler: LRScheduler,
            dropout_scheduler: DropoutScheduler,
            log_interval: int,
            logger: Logger,
    ) -> "Trainer":
        trainer = Trainer._init_base(model, train_loader, val_loader, device, tokenizer, log_interval, logger)

        trainer.optimizer = optimizer
        trainer.lr_scheduler = lr_scheduler
        trainer.dropout_scheduler = dropout_scheduler
        trainer.history = History()
        trainer.best_val_loss = float("inf")
        trainer.start_epoch = 1

        trainer.logger.info("è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        trainer._log_basic()

        return trainer

    @staticmethod
    def from_checkpoint(
            checkpoint_path: Path,
            model: ColorModel,
            train_loader: DataLoader,
            val_loader: DataLoader,
            device: device_type,
            tokenizer: Tokenizer,
            log_interval: int,
            logger: Logger,
            optimizer: Optimizer,
            lr_scheduler: LRScheduler,
            dropout_scheduler: DropoutScheduler,
            load_optimizer: bool = True,
            load_lr_scheduler: bool = True,
            load_dp_scheduler: bool = True,
    ) -> "Trainer":
        if not checkpoint_path.exists():
            raise FileNotFoundError(f"Checkpointæ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path.absolute()}")

        trainer = Trainer._init_base(model, train_loader, val_loader, device, tokenizer, log_interval, logger)
        trainer.optimizer = optimizer
        trainer.lr_scheduler = lr_scheduler
        trainer.dropout_scheduler = dropout_scheduler

        checkpoint = torch.load(checkpoint_path)

        trainer.model.load_state_dict(checkpoint["model_state"])

        if load_optimizer:
            try:
                trainer.optimizer.load_state_dict(checkpoint["optimizer_state"])
            except Exception as e:
                logger.warning(f"æ— æ³•åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€: {e}")

        if load_lr_scheduler:
            try:
                trainer.lr_scheduler.load_state_dict(checkpoint["lr_scheduler_state"])
            except Exception as e:
                logger.warning(f"æ— æ³•åŠ è½½å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€: {e}")

        if load_dp_scheduler:
            try:
                trainer.dropout_scheduler.load_state_dict(checkpoint["dp_scheduler_state"])
            except Exception as e:
                logger.warning(f"æ— æ³•åŠ è½½ä¸¢å¼ƒç‡è°ƒåº¦å™¨çŠ¶æ€: {e}")

        trainer.history = History.from_dict(checkpoint["history"])
        trainer.best_val_loss = checkpoint["best_val_loss"]
        trainer.start_epoch = checkpoint["start_epoch"] + 1

        trainer.logger.info("è®­ç»ƒå™¨æ¢å¤å®Œæˆ")
        trainer._log_basic()

        return trainer

    @staticmethod
    def save_checkpoint(checkpoint_name: str):
        trainer = object.__new__(Trainer)
        checkpoint_path = Trainer.checkpoint_dir / f"{checkpoint_name}.pt"

        if not checkpoint_path.exists():
            raise FileNotFoundError(f"Checkpointæ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path.absolute()}")

        checkpoint = torch.load(checkpoint_path)
        model_path = trainer.model_dir / f"color_semantic_{version("flow-tint")}.pt"

        torch.save(checkpoint["model_state"], model_path)

    def _log_basic(self):
        self.logger.info(f"å­¦ä¹ ç‡: {self.optimizer.param_groups[0]['lr']}")
        self.logger.info(f"æ‰¹æ¬¡å¤§å°: {cast(LengthGroupedBatchSampler, self.train_loader.batch_sampler).batch_size}")
        self.logger.info(f"è®­ç»ƒæ‰¹æ¬¡æ•°: {len(self.train_loader)}")
        self.logger.info(f"éªŒè¯æ‰¹æ¬¡æ•°: {len(self.val_loader)}")

    def train_epoch(self, epoch: int):
        """ä¸€ä¸ªè®­ç»ƒepoch"""

        self.model.train()
        total_loss = 0.0
        total_norm = 0.0
        all_token_acc = []
        all_seq_acc = []

        progress = tqdm(self.train_loader, f"Epoch {epoch} [Train]")

        for batch_idx, batch in enumerate(progress):
            batch: dict[str, Tensor]

            src = batch["src"].to(self.device)
            tgt_in = batch["tgt_in"].to(self.device)
            tgt_out = batch["tgt_out"].to(self.device)
            src_padding_mask = batch["src_padding_mask"].to(self.device)
            tgt_padding_mask = batch["tgt_padding_mask"].to(self.device)

            # ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªepochè¾“å‡ºè¯¦ç»†ä¿¡æ¯
            if epoch == 1 and batch_idx == 0 or epoch == 50 and batch_idx == len(progress):
                print(f"\n=== è®­ç»ƒæ•°æ®è¯Šæ–­ ===")
                print(f"Batch shapes - src: {src.shape}, tgt_in: {tgt_in.shape}, tgt_out: {tgt_out.shape}")
                print(f"Source tokens: {src[0].tolist()}")
                print(f"Target input: {tgt_in[0].tolist()}")
                print(f"Target output: {tgt_out[0].tolist()}")
                print(f"Source text: {self.tokenizer.decode(src[0].tolist())}")
                print(f"Target text: {self.tokenizer.decode(tgt_out[0].tolist())}")
                print(f"Padding masks - src: {src_padding_mask[0]}, tgt: {tgt_padding_mask[0]}")

            # å‰å‘ä¼ æ’­
            logits = self.model(
                src,
                tgt_in,
                src_key_padding_mask=src_padding_mask,
                tgt_key_padding_mask=tgt_padding_mask,
            )

            # ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªepochè¾“å‡ºæ¨¡å‹è¾“å‡ºè¯Šæ–­
            if epoch == 1 and batch_idx == 0 or epoch == 50 or batch_idx == len(progress):
                print(f"\n=== æ¨¡å‹è¾“å‡ºè¯Šæ–­ ===")
                print(f"Logits shape: {logits.shape}")
                print(f"Logits range: [{logits.min().item():.3f}, {logits.max().item():.3f}]")
                print(f"Logits mean: {logits.mean().item():.3f}, std: {logits.std().item():.3f}")

                # é¢„æµ‹çš„token
                predictions = logits.argmax(dim=-1)
                print(f"Predicted tokens: {predictions[0].tolist()}")
                print(f"Predicted text: {self.tokenizer.decode(predictions[0].tolist())}")

                # è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ
                probs = F.softmax(logits[0, 0], dim=-1)
                top_probs, top_indices = torch.topk(probs, 5)
                print(f"Top 5 probabilities for first token:")
                for i, (prob, idx) in enumerate(zip(top_probs, top_indices)):
                    token_text = self.tokenizer.decode([idx.item()])
                    print(f"  {i + 1}. {token_text} (id={idx.item()}): {prob.item():.4f}")

            # è®¡ç®—æŸå¤±
            loss = compute_loss(logits, tgt_out, tgt_padding_mask)

            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()

            # æ¢¯åº¦è£å‰ª
            total_norm = clip_grad_norm_(self.model.parameters(), 1).item()

            # ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªepochè¾“å‡ºæ¢¯åº¦è¯Šæ–­
            if epoch == 1 and batch_idx == 0 or epoch == 50 or batch_idx == len(progress):
                # æ¢¯åº¦è¯Šæ–­
                print(f"\n=== æ¢¯åº¦è¯Šæ–­ ===")
                print(f"Gradient norm before clipping: {total_norm:.4f}")

                # æ£€æŸ¥å„å±‚æ¢¯åº¦
                grad_norms = {}
                for name, param in self.model.named_parameters():
                    if param.grad is not None:
                        grad_norms[name] = param.grad.norm().item()

                print("Gradient norms by layer:")
                for name, norm in sorted(grad_norms.items(), key=lambda x: x[1], reverse=True)[:5]:
                    print(f"  {name}: {norm:.4f}")

            total_loss += loss.item()

            self.optimizer.step()

            metrics = compute_metrics(logits, tgt_out, tgt_padding_mask)
            all_token_acc.append(metrics["token_accuracy"])
            all_seq_acc.append(metrics["sequence_accuracy"])

            # æ›´æ–°è¿›åº¦æ¡
            if batch_idx % self.log_interval == 0:
                progress.set_postfix({
                    "loss": f"{loss.item():.5f}",
                    "token_acc": f"{metrics['token_accuracy']:.3f}",
                    "seq_acc": f"{metrics['sequence_accuracy']:.3f}",
                    "grad_norm": f"{total_norm:.3f}",
                })

        # è®¡ç®—epochæŒ‡æ ‡
        avg_loss = total_loss / len(self.train_loader)
        avg_token_acc = sum(all_token_acc) / len(all_token_acc)
        avg_seq_acc = sum(all_seq_acc) / len(all_seq_acc)

        epoch_metrics = {
            "avg_token_accuracy": avg_token_acc,
            "avg_sequence_accuracy": avg_seq_acc,
        }

        return avg_loss, total_norm, epoch_metrics

    @torch.no_grad()
    def validate(self, epoch: int):
        """éªŒè¯æ¨¡å‹"""

        self.model.eval()
        total_loss = 0
        all_token_acc = []
        all_seq_acc = []

        progress = tqdm(self.val_loader, f"Epoch {epoch} [Val]")

        for batch in progress:
            batch: dict[str, Tensor]

            src = batch["src"].to(self.device)
            tgt_in = batch["tgt_in"].to(self.device)
            tgt_out = batch["tgt_out"].to(self.device)
            src_padding_mask = batch["src_padding_mask"].to(self.device)
            tgt_padding_mask = batch["tgt_padding_mask"].to(self.device)

            logits = self.model(
                src,
                tgt_in,
                src_key_padding_mask=src_padding_mask,
                tgt_key_padding_mask=tgt_padding_mask,
            )

            loss = compute_loss(logits, tgt_out, tgt_padding_mask)
            total_loss += loss.item()

            metrics = compute_metrics(logits, tgt_out, tgt_padding_mask)
            all_token_acc.append(metrics["token_accuracy"])
            all_seq_acc.append(metrics["sequence_accuracy"])

        avg_loss = total_loss / len(self.val_loader)
        avg_token_acc = sum(all_token_acc) / len(all_token_acc)
        avg_seq_acc = sum(all_seq_acc) / len(all_seq_acc)

        epoch_metrics = {
            "avg_token_accuracy": avg_token_acc,
            "avg_sequence_accuracy": avg_seq_acc,
        }

        return avg_loss, epoch_metrics

    def _save_emergency_report(self, reason: str = "æ‰‹åŠ¨ä¸­æ–­", save_checkpoint: bool = True):
        """ä¿å­˜ç´§æ€¥è®­ç»ƒæŠ¥å‘Š"""
        emergency_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        try:
            # ä¿å­˜History
            self.history.save_to_file(self.log_dir / f"training_{emergency_timestamp}_emergency_history.json")

            # å°è¯•ä¿å­˜å½“å‰æ¨¡å‹çŠ¶æ€ (å³ä½¿ä¸å®Œæ•´)
            if save_checkpoint and hasattr(self, 'optimizer') and hasattr(self, 'scheduler'):
                current_epoch = len(self.history.epochs)

                torch.save(
                    {
                        "model_state": self.model.state_dict(),
                        "optimizer_state": self.optimizer.state_dict(),
                        "lr_scheduler_state": self.lr_scheduler.state_dict(),
                        "dp_scheduler_state": self.dropout_scheduler.state_dict(),
                        "history": self.history.to_dict(),
                        "best_val_loss": self.best_val_loss,
                        "start_epoch": current_epoch,
                        "is_emergency_save": True,
                        "emergency_reason": reason,
                    },
                    self.checkpoint_dir / f"emergency_{emergency_timestamp}.pt",
                )

                print(f"âš ï¸  ç´§æ€¥ä¿å­˜æ¨¡å‹checkpoint (å¯èƒ½ä¸å®Œæ•´): checkpoints/emergency_{emergency_timestamp}.pt")

            # ç”Ÿæˆç´§æ€¥æŠ¥å‘Š
            emergency_report = f"è®­ç»ƒ{reason}æŠ¥å‘Š\n"
            emergency_report += f"ä¸­æ–­æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
            emergency_report += f"ä¸­æ–­åŸå› : {reason}\n"
            emergency_report += f"å·²å®Œæˆepochæ•°: {len(self.history.epochs)}\n"
            emergency_report += f"æœ€åä¿å­˜çš„æœ€ä½³æŸå¤±: {self.best_val_loss:.5f}\n"
            emergency_report += "âš ï¸  æ³¨æ„: ç´§æ€¥ä¿å­˜çš„æ¨¡å‹å¯èƒ½åŒ…å«ä¸å®Œæ•´çš„è®­ç»ƒçŠ¶æ€\n"
            emergency_report += "=" * 60 + "\n\n"
            emergency_report += self.history.generate_training_report()

            with open(
                    self.log_dir / f"training_{emergency_timestamp}_emergency_report.txt",
                    'w',
                    encoding='utf-8',
            ) as f:
                f.write(emergency_report)

            print(f"\nğŸ’¾ ç´§æ€¥ä¿å­˜å®Œæˆ: logs/training_{emergency_timestamp}_emergency_*")
            if self.logger:
                self.logger.info(f"ç´§æ€¥ä¿å­˜è®­ç»ƒæ•°æ®: {reason} | å·²å®Œæˆepoch: {len(self.history.epochs)}")

        except Exception as e:
            print(f"âŒ ç´§æ€¥ä¿å­˜å¤±è´¥: {e}")
            # è‡³å°‘å°è¯•ä¿å­˜History
            try:
                self.history.save_to_file(self.log_dir / f"training_{emergency_timestamp}_history_only.json")
                print(f"ğŸ“ è‡³å°‘ä¿å­˜äº†è®­ç»ƒå†å²æ•°æ®")
            except:
                print(f"ğŸ’¥ è¿å†å²æ•°æ®éƒ½æ— æ³•ä¿å­˜")

    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""

        # è®¾ç½®ä¿¡å·å¤„ç†å™¨
        def signal_handler(signum, _):
            print(f"\nğŸ›‘ æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å· ({signum})")
            self._save_emergency_report("æ‰‹åŠ¨ä¸­æ–­")
            sys.exit(0)

        signal.signal(signal.SIGINT, signal_handler)  # Ctrl+C
        signal.signal(signal.SIGTERM, signal_handler)  # IDEåœæ­¢æŒ‰é’®

        if self.logger:
            self.logger.info(f"å¼€å§‹è®­ç»ƒï¼Œå…±{self.epochs - (self.start_epoch - 1)}ä¸ªepoch")

        try:
            for epoch in range(self.start_epoch, self.epochs + 1):
                epoch_start_time = datetime.now()

                print(f"\n{"=" * 60}")
                print(f"Epoch {epoch}/{self.epochs}")
                print(f"=" * 60)

                train_loss, total_norm, train_metrics = self.train_epoch(epoch)
                val_loss, val_metrics = self.validate(epoch)

                # è°ƒç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨
                old_lr = self.optimizer.param_groups[0]['lr']
                if isinstance(self.lr_scheduler, ReduceLROnPlateau):
                    self.lr_scheduler.step(val_loss)
                else:
                    self.lr_scheduler.step()
                new_lr = self.optimizer.param_groups[0]['lr']

                epoch_time = datetime.now() - epoch_start_time

                print(f"\nTrain Loss: {train_loss:.5f}, Val Loss: {val_loss:.5f}")
                print(
                    f"Train Token Accuracy: {train_metrics["avg_token_accuracy"]:.5f}, ",
                    f"Train Sequence Accuracy: {train_metrics["avg_sequence_accuracy"]:.5f}",
                )
                print(
                    f"Val Token Accuracy: {val_metrics["avg_token_accuracy"]:.5f}, ",
                    f"Val Sequence Accuracy: {val_metrics["avg_sequence_accuracy"]:.5f}",
                )

                # è°ƒç”¨dropoutè°ƒåº¦å™¨
                old_dp = self.dropout_scheduler.current_dropout
                train_decreasing, val_increasing = self.history.detect_overfitting()
                self.dropout_scheduler.step(train_decreasing, val_increasing)
                new_dp = self.dropout_scheduler.current_dropout

                self.history.add_epoch(
                    epoch=epoch,
                    epoch_time=epoch_time.total_seconds(),
                    train_loss=train_loss,
                    val_loss=val_loss,
                    train_metrics=train_metrics,
                    val_metrics=val_metrics,
                    learning_rate=new_lr,
                    dropout=new_dp,
                    gradient_norm=total_norm,
                    early_stopping_counter=self.early_stopping_counter,
                )

                # è¯¦ç»†æ—¥å¿—è®°å½•
                if self.logger:
                    lr = f"å­¦ä¹ ç‡: {new_lr:.2e}" + \
                         (f" (ä» {old_lr:.2e} è°ƒæ•´)" if old_lr != new_lr else "")
                    dp = f"ä¸¢å¼ƒç‡: {new_dp:.2f}" + \
                         (f" (ä» {old_dp:.2f} è°ƒæ•´)" if old_dp != new_dp else "")

                    self.logger.info(
                        f"Epoch {epoch:2d}/{self.epochs} | "
                        f"æ—¶é—´: {epoch_time.total_seconds():.1f}s | "
                        f"è®­ç»ƒæŸå¤±: {train_loss:.5f} | "
                        f"éªŒè¯æŸå¤±: {val_loss:.5f} | "
                        f"è®­ç»ƒtokenå‡†ç¡®ç‡: {train_metrics['avg_token_accuracy']:.4f} | "
                        f"è®­ç»ƒåºåˆ—å‡†ç¡®ç‡: {train_metrics['avg_sequence_accuracy']:.4f} | "
                        f"éªŒè¯tokenå‡†ç¡®ç‡: {val_metrics['avg_token_accuracy']:.4f} | "
                        f"éªŒè¯åºåˆ—å‡†ç¡®ç‡: {val_metrics['avg_sequence_accuracy']:.4f} | "
                        f"æ¢¯åº¦: {total_norm} | "
                        f"{lr} | "
                        f"{dp}"
                    )

                # ä¿å­˜æœ€ä½³æ¨¡å‹å’Œæ—©åœæ£€æŸ¥
                if val_loss < self.best_val_loss:
                    self.best_val_loss = val_loss
                    self.early_stopping_counter = 0

                    now = datetime.now().strftime("%Y%m%d_%H%M%S")

                    # ä¿å­˜checkpoint
                    torch.save(
                        {
                            "model_state": self.model.state_dict(),
                            "optimizer_state": self.optimizer.state_dict(),
                            "lr_scheduler_state": self.lr_scheduler.state_dict(),
                            "dp_scheduler_state": self.dropout_scheduler.state_dict(),
                            "history": self.history.to_dict(),
                            "best_val_loss": val_loss,
                            "start_epoch": epoch,
                        },
                        self.checkpoint_dir / f"color_semantic_{now}.pt",
                    )

                    # åŒæ­¥ä¿å­˜Historyæ–‡ä»¶
                    self.history.save_to_file(self.log_dir / f"training_{now}_history.json")

                    print(f"âœ“ Saved best model(val loss: {self.best_val_loss:.5f})")
                    if self.logger:
                        self.logger.info(f"ä¿å­˜æœ€ä½³æ¨¡å‹ | epoch: {epoch} | éªŒè¯æŸå¤±: {self.best_val_loss:.5f}")
                else:
                    self.early_stopping_counter += 1
                    if self.logger:
                        self.logger.info(
                            f"éªŒè¯æŸå¤±æœªæ”¹å–„ | æ—©åœè®¡æ•°: {self.early_stopping_counter}/{self.early_stopping_patience}"
                        )

                    if self.early_stopping_counter >= self.early_stopping_patience:
                        if self.logger:
                            self.logger.info(f"æ—©åœè§¦å‘ | æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.5f}")
                        print(f"Early stopping at epoch {epoch}")
                        break

        except KeyboardInterrupt:
            print(f"\nâš ï¸  è®­ç»ƒè¢«æ‰‹åŠ¨ä¸­æ–­ (Ctrl+C)")
            self._save_emergency_report("Ctrl+Cä¸­æ–­")
            return
        except Exception as e:
            print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}")
            self._save_emergency_report(f"å¼‚å¸¸ä¸­æ–­: {e}")
            raise

        # è®­ç»ƒæ­£å¸¸ç»“æŸï¼Œä¿å­˜æœ€ç»ˆæŠ¥å‘Š
        final_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.history.save_training_report(self.log_dir / f"training_{final_timestamp}_report.txt")

        if self.logger:
            self.logger.info(f"è®­ç»ƒå®Œæˆ | æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.5f}")
            self.logger.info(f"è®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜: logs/training_{final_timestamp}_report.txt")
